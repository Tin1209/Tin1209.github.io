---
title: "[딥러닝 기초] 회귀 분석" 
categories:
- Deep Learning 
tags: 
- Deep Learning
use_math: true
---



# 회귀 분석 

__회귀 분석(regression analysis)__ 에서 회귀라는 단어는 옛날 상태로 돌아간다는 뜻이다. 회귀 분석이라는 말은 영국의 유전학자 프랜시스 골턴이 최초로 사용했는데, 부모와 자녀의 키의 상관관계를 연구하다가 세대의 평균적인 키가 커지거나 작아지지 않고, 조사 집단의 평균값으로 되돌아가려는 경향이 있다 라는 가설을 세우면서 사용한 말이다. 통계학에서는 연속성 변수 사이의 관계도를 나타낸 모형을 구한 뒤, 적합도를 측정하는 방식을 회귀 분석이라고 하며, 입력값에 대해 미지의 변수 값을 추정하는 데 쓰인다.

이번 장에서는 회귀 분석중에서도 출력값 y가 입력값 x에 대해서 선형적인 관계를 가질 것이라고 가정하는 선형 회귀에 대해서 다룬다. 



##  선형 연산 

------

입력 벡터 하나로부터 출력 벡터 하나를 바로 얻어내는 가장 기본적인 구조이다. 출력 $\mathbf{y}$에 대해서 $\mathbf{y} = \mathbf{x} \mathbf{w} + \mathbf{b}$ 형태로 나타낼 수 있으며, 퍼셉트론의 개수가 하나이고, 입력 벡터의 길이가 m개인 경우 다음과 같이 나타낼 수 있다. 
$$
y = \begin{bmatrix} x_1 & x_2 & \cdots & x_m \end{bmatrix} \begin{bmatrix}
w_{1} \\
w_{2} \\
\vdots \\
w_{m} \end{bmatrix} + 
b
$$
  이렇게 입력 성분의 일차식으로 표현되는 계산 과정을 선형 연산이라 부른다. 

여기서 __가중치(weight)__ 와 __편향(bias)__ 을 __parameter__ 라고 표현한다. 이는 학습을 하면서 계속 업데이트 시켜줘야하는 값이다.  

<br/><br/>

##  미니 배치

-----

딥러닝에서는 일반적으로 신경망이 여러 개의 데이터를 한꺼번에 처리하는데 이를 __미니배치(mini batch)__ 라고 한다. 

이렇게 데이터를 나누어서 처리하게 되면 입력값이 위와는 조금 달라진다. 형태는 아래와 같다. 


$$
\begin{bmatrix}
y_{11} & y_{12} & \cdots & y_{1n} \\
y_{21} & y_{22} & \cdots & y_{2n} \\
\vdots & \vdots & \ddots & \cdots \\
y_{m1} & y_{m2} & \cdots & y_{mn}
\end{bmatrix}
=
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1m} \\
x_{21} & x_{22} & \cdots & x_{2m} \\ 
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mm}
\end{bmatrix}
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1n} \\
w_{21} & w_{22} & \cdots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
w_{m1} & w_{m2} & \cdots & w_{mn}
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
$$
 여기서 $ n $ 이 의미하는 것은 퍼셉트론의 개수이다. 퍼셉트론의 개수는 출력값의 개수를 의미하며, 보통 퍼셉트론의 개수를 먼저 정하는 것이 아니라 원하는 출력값의 특성의 개수를 설정하고 그에 맞춰 퍼셉트론의 개수를 정해준다. 데이터를 나눠서 보자면 미니배치는 행 단위, 퍼셉트론은 열 단위로 보면된다. 즉, 첫번째 미니배치는 입력 행렬의 첫번째 행이고, 세번째 퍼셉트론의 가중치를 나타내는 부분은 가중치 행렬의 세번째 열인 것이다.   

출력값의 특성 개수는 퍼셉트론의 개수와 같고, 출력값의 개수는 입력값의 미니 배치 크기와 같다. 즉 미니 배치의 크기가 a이고 입력값의 특성 개수는 b, 원하는 출력값의 특성 개수가 c인 경우 각각의 행렬의 크기를 다음과 같이 쓸 수 있다.   
$$
(\mathbf{a} \times \mathbf{b}) \times (\mathbf{b} \times \mathbf{c}) = (\mathbf{a} \times \mathbf{c})
$$


<br/><br/>



##  용어 정리 

-----

공부를 하다보면 용어가 너무 헷갈린다. 처음에 배치를 이해하는게 정말 힘들었다.  

- __에포크(epoch)__ : 사람마다 부르는 방법이 참 다양한데 나는 에포크라고 부른다. 학습 데이터 전체에 대해서 한 번 처리하는 것을 1epoch 라고 한다. 한 에포크를 돌 때마다 파라미터들은 여러번 업데이트 될 수도 있다. 
- __미니배치(mini-batch)__ : 데이터를 처리할 때 효율을 높여주기 위해서 전체 데이터를 작은 단위로 나누어 한 에포크에서 데이터를 여러번 나눠 학습을 진행하는 단위를 말한다. 예를 들어 입력값이 사진의 형태로 들어올 때, 사진이 가지고 있는 정보는 픽셀의 위치와 그 위치에서의 RGB 값일 것이다. 그렇다면 총 3개의 채널을 가지고 있을 것이다. 여기에서 만약 이미지가 5개가 들어왔는데, 이것을 일괄처리 해버리면 각각 이미지의 특성을 살릴 수 없게된다. 여기서 batch_size를 15로 설정하면 전체 데이터를 15개의 미니배치로 쪼개어 학습을 진행한다는 것이므로, 각각의 이미지의 R, G, B 채널에 대해 따로 학습을 진행하게 된다. 보통은 이미지 하나에 대해 학습을 하기 때문에 이러한 경우에는 한 미니배치에 RGB 세 채널이 모두 들어가게끔 batch_size를 5로 설정해줄 것이다. 
- __학습률(Learning rate)__ : 경사하강법을 이용할 때 파라미터를 업데이트 해주는 정도를 설정하는 변수이다. 

에포크와 미니배치는 학습 도중에는 변하지 않는 값들이다. 하지만 신경망 구조나 학습의 결과에 영향을 미치기때문에 이러한 변수들을 __하이퍼파라미터(Hyper parameter)__ 라고 한다. 

딥러닝 모델들을 개발하다보면 여러 가지 값들이 나오는데 이것들을 크게 네 가지로 나눌 수 있다. 

- 입력 데이터: 외부에서 주어지는 값으로 신경망 개발자 입장에서는 직접 손댈 수 없는 데이터이다. 

- 중간 계산 결과: 학습 도중 레이어를 거쳐 나온 중간 결과값들을 말한다. 학습 과정에서 모델의 성능의 평가 지표로 판단되기도 하고, 문제가 생겼을 때 확인을 해야할 값이기도 하지만 프로그램의 실행 결과물이므로 좋은 결과물이 나오게끔 할 수는 있지만, 직접 값을 바꾸지는 못한다. 

- 파라미터: 학습이 진행됨에 따라 계속해서 값이 바뀌는 변수를 말한다. 앞서 소개한 파라미터로는 가중치와 편향이 있다. 이 또한 파라미터의 업데이트 설정을 바꿔줄 수는 있지만 개발자가 직접 파라미터를 바꾸지는 않는다. 

- 하이퍼파라미터: 학습을 진행하기 전, 개발자가 미리 지정해두는 변수로 학습 도중에는 바뀌지 않지만 학습의 결과에 유의미한 영향을 끼치는 변수를 말한다. 위에서 소개한 학습률, 에포크, 배치사이즈 등이 이에 해당한다. 

  <br/><br/>

##  신경망의 세 가지 기본 출력 유형 

------

신경망에는 총 세가지 유형이 있는데 간단하게 짚고 넘어가자. 

1. 회귀 분석: 어떤 특징값 하나를 숫자로 추정하여 출력하는 것. 미래의 주식 가격을 예측하거나, 학생들의 공부량을 보고 시험 성적을 예측하는 등 특정 숫자를 추정하는데 쓰인다. 
2. 이진 판단: True or False 문제이다. 가짜 데이터와 진짜 데이터를 구분하여 0 또는 1을 출력하는 문제가 여기에 속한다. 
3. 선택 분류: 몇 가지의 후보 항목 중 하나를 골라 선택 결과를 출력한다. 쉽게 생각하면 객관식 문제라고 보면된다. 동물 사진을 주고 선택지에는 강아지, 고양이, 사자, 토끼 등이 있는 상황. 

<br/><br/>

##  회귀 분석의 과정

---

### 1) 손실 함수 지정 

>  이제부터 우리가 진행할 학습은 파라미터 $ w $ 와 $ b $의 적절한 값을 찾는 것이다. 어떤 $w$와 $b$에 대해서 나온 예측값 $\hat{y}$ 가 얼마나 정답에 근접했는지를 수치화 하기 위해 __손실 함수(loss function)__ 을 사용한다. 회귀 분석에서는 손실 함수로 __평균제곱오차(MSE)__ 함수를 사용할 것이며, 이것은 예측값 $\hat{y}$와 정답값 $y$의 차이를 제곱한 것들의 평균을 의미한다. 



<br/><br/>



### 2) 경사하강법과 역전파 

> 경사하강법이란 함수의 기울기를 계산하면서, 기울기에 따라 함숫값이 낮아지는 방향으로 이동하는 알고리즘이다. 이 과정에는 __forward propagation__ 과 __backward propagation__ 의 두 과정이 있는데, forward propagation은 입력값에 대해 신경망 구조를 통과하면서 현재의 파라미터 값을 이용해 위에서 소개한 손실 함수의 값을 계산하는 과정을 말한다. backward propagation은 계산 과정을 거슬러 올라가면서 각각의 변수들이 손실값에 미친 영향들을 기울기의 값으로 구하는 과정이다.  인공신경망의 최종 목표는 손실 함수의 극솟값을 찾는 것이다. 파라미터를 업데이트 하는 식은 다음과 같다. 
>
> $$ x_{t+1} = x_{t} - lr \times \frac{\partial{lossfunc}}{\partial{x}}$$ 
>
> 여기서 파라미터가 여러개가 쓰일 수 있기 때문에 편미분을 해준다. 각각의 변수들이 손실 함수에 끼치는 영향에 대해 각각 다른 비율로 업데이트를 해줘야하기 때문이다. 



<br/><br/>



### 3) 편미분과 손실 기울기의 계산 

> 위에서 구한 파라미터 업데이트 식에서 손실함수를 파라미터로 미분한 값은 신경망이 깊어질수록 구하기가 힘들어진다. 그래서 파라미터로 손실 함수를 편미분 한 값을 구할때는 __Chain Rule__ 을 이용한다. 

 ![chain_rule](/img/chain_rule.jpeg)

> 그림에서 x가 입력값이고, 신경망을 통해 y가 나왔을 때, 그 y 값을 이용해 Loss function의 값을 계산해준다. 여기서 L을 바로 x로 미분한 값을 찾기가 힘들기 때문에 연쇄법칙(chain rule)을 사용하여 미분값을 구해준다. 



<br/><br/>

* __원 핫 벡터(one-hot-vector)__ : 하나 빼고 나머지는 모두 0의 값을 가지는 벡터를 말한다. 예를 들어 고양이의 눈 색깔에 대한 정보가 "노란색, 파란색, 초록색" 으로 주어졌다면, 이를 처리하기 위해 눈 색깔에 대한 특징값을 세 개를 만들고, 눈이 노란색이면 [1, 0, 0]을, 파란색이면 [0, 1, 0], 초록색이면 [0, 0, 1]로 나타내는 경우가 있다. 



