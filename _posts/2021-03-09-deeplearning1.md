---
title: 알짜딥러닝 1강 
categories:

- Deep Learning 
tags: 
- Deep Learning
use_math: true
---



## 회귀 분석 



### 1. 선형 연산 

------

입력 벡터 하나로부터 출력 벡터 하나를 바로 얻어내는 가장 기본적인 구조이다. 출력 $\mathbf{y}$에 대해서 $\mathbf{y} = \mathbf{x} \mathbf{w} + \mathbf{b}$ 형태로 나타낼 수 있으며, 퍼셉트론의 개수가 하나이고, 입력 벡터의 길이가 m개인 경우 다음과 같이 나타낼 수 있다. 
$$
y = \begin{bmatrix} x_1 & x_2 & \cdots & x_m \end{bmatrix} \begin{bmatrix}
w_{1} \\
w_{2} \\
\vdots \\
w_{m} \end{bmatrix} + 
b
$$
  이렇게 입력 성분의 일차식으로 표현되는 계산 과정을 선형 연산이라 부른다. 

여기서 __가중치(weight)__ 와 __편향(bias)__ 을 __parameter__ 라고 표현한다. 이는 학습을 하면서 계속 업데이트 시켜줘야하는 값이다.  

<br/><br/>

### 2. 미니 배치

-----

딥러닝에서는 일반적으로 신경망이 여러 개의 데이터를 한꺼번에 처리하는데 이를 __미니배치(mini batch)__ 라고 한다. 이렇게 데이터를 나누어서 처리하게 되면 입력값이 위와는 조금 달라진다. 형태는 아래와 같다. 
$$
\begin{bmatrix}
y_{11} & y_{12} & \cdots & y_{1n} \\
y_{21} & y_{22} & \cdots & y_{2n} \\
\vdots & \vdots & \ddots & \cdots \\
y_{m1} & y_{m2} & \cdots & y_{mn}
\end{bmatrix}
=
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1m} \\
x_{21} & x_{22} & \cdots & x_{2m} \\ 
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mm}
\end{bmatrix}
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1n} \\
w_{21} & w_{22} & \cdots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
w_{m1} & w_{m2} & \cdots & w_{mn}
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n
\end{bmatrix}
$$
 여기서 $ n $ 이 의미하는 것은 퍼셉트론의 개수이다. 퍼셉트론의 개수는 출력값의 개수를 의미하며, 보통 퍼셉트론의 개수를 먼저 정하는 것이 아니라 원하는 출력값의 특성의 개수를 설정하고 그에 맞춰 퍼셉트론의 개수를 정해준다. 데이터를 나눠서 보자면 미니배치는 행 단위, 퍼셉트론은 열 단위로 보면된다. 즉, 첫번째 미니배치는 입력 행렬의 첫번째 행이고, 세번째 퍼셉트론의 가중치를 나타내는 부분은 가중치 행렬의 세번째 열인 것이다.   

출력값의 특성 개수는 퍼셉트론의 개수와 같고, 출력값의 개수는 입력값의 미니 배치 크기와 같다. 즉 미니 배치의 크기가 a이고 입력값의 특성 개수는 b, 원하는 출력값의 특성 개수가 c인 경우 각각의 행렬의 크기를 다음과 같이 쓸 수 있다.   
$$
(\mathbf{a} \times \mathbf{b}) \times (\mathbf{b} \times \mathbf{c}) = (\mathbf{a} \times \mathbf{c})
$$
   

<br/><br/>

 ### 3. 용어 정리 

-----

공부를 하다보면 용어가 너무 헷갈린다. 처음에 배치를 이해하는게 정말 힘들었다.  

- __에포크(epoch)__ : 사람마다 부르는 방법이 참 다양한데 나는 에포크라고 부른다. 학습 데이터 전체에 대해서 한 번 처리하는 것을 1epoch 라고 한다. 한 에포크를 돌 때마다 파라미터들은 여러번 업데이트 될 수도 있다. 
- __미니배치(mini-batch)__ : 데이터를 처리할 때 효율을 높여주기 위해서 전체 데이터를 작은 단위로 나누어 한 에포크에서 데이터를 여러번 나눠 학습을 진행하는 단위를 말한다. 예를 들어 입력값이 사진의 형태로 들어올 때, 사진이 가지고 있는 정보는 픽셀의 위치와 그 위치에서의 RGB 값일 것이다. 그렇다면 총 3개의 채널을 가지고 있을 것이다. 여기에서 만약 이미지가 5개가 들어왔는데, 이것을 일괄처리 해버리면 각각 이미지의 특성을 살릴 수 없게된다. 여기서 batch_size를 15로 설정하면 전체 데이터를 15개의 미니배치로 쪼개어 학습을 진행한다는 것이므로, 각각의 이미지의 R, G, B 채널에 대해 따로 학습을 진행하게 된다. 보통은 이미지 하나에 대해 학습을 하기 때문에 이러한 경우에는 한 미니배치에 RGB 세 채널이 모두 들어가게끔 batch_size를 5로 설정해줄 것이다. 

에포크와 미니배치는 학습 도중에는 변하지 않는 값들이다. 하지만 신경망 구조나 학습의 결과에 영향을 미치기때문에 이러한 변수들을 __하이퍼파라미터(Hyper parameter)__ 라고 한다. 

<br/><br/>

### 4. 신경망의 세 가지 기본 출력 유형 

------

신경망에는 총 세가지 유형이 있는데 간단하게 짚고 넘어가자. 

1. 회귀 분석: 어떤 특징값 하나를 숫자로 추정하여 출력하는 것. 미래의 주식 가격을 예측하거나, 학생들의 공부량을 보고 시험 성적을 예측하는 등 특정 숫자를 추정하는데 쓰인다. 
2. 이진 판단: True or False 문제이다. 가짜 데이터와 진짜 데이터를 구분하여 0 또는 1을 출력하는 문제가 여기에 속한다. 
3. 선택 분류: 몇 가지의 후보 항목 중 하나를 골라 선택 결과를 출력한다. 쉽게 생각하면 객관식 문제라고 보면된다. 동물 사진을 주고 선택지에는 강아지, 고양이, 사자, 토끼 등이 있는 상황. 

<br/><br/>



### 5. 회귀 분석의 과정  

----

#### 1) 손실 함수 지정 

>  이제부터 우리가 진행할 학습은 파라미터 $ w $ 와 $ b $의 적절한 값을 찾는 것이다. 어떤 $w$와 $b$에 대해서 나온 예측값 $\hat{y}$ 가 얼마나 정답에 근접했는지를 수치화 하기 위해 __손실 함수(loss function)__ 을 사용한다. 회귀 분석에서는 손실 함수로 __평균제곱오차(MSE)__ 함수를 사용할 것이며, 이것은 예측값 $\hat{y}$와 정답값 $y$의 차이를 제곱한 것들의 평균을 의미한다. 



<br/>



#### 2) 경사하강법과 역전파 

> 경사하강법이란 



